{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598cd377",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "911177a5",
   "metadata": {},
   "source": [
    "# Affinity Propagation and Agglomerative Clustering\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the implementation of **Affinity Propagation (AP)** and **Agglomerative Clustering** on the tweets data. It covers the following steps:\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Import Libraries**\n",
    "   - Necessary libraries are imported, including those for clustering, dimensionality reduction, and evaluation.\n",
    "\n",
    "2. **Load and Prepare Data**\n",
    "   - The TF-IDF matrix is loaded from a saved file (`tfidf_matrix.pkl`).\n",
    "   - Dimensionality reduction is applied using TruncatedSVD to reduce the matrix to 100 components.\n",
    "\n",
    "3. **Chunk Processing**\n",
    "   - The data is processed in chunks for Affinity Propagation to handle large datasets efficiently.\n",
    "   - The `chunk_data` function splits the data into manageable sizes.\n",
    "\n",
    "4. **Affinity Propagation Clustering**\n",
    "   - Affinity Propagation is applied to each data chunk.\n",
    "   - Labels from each chunk are collected and combined.\n",
    "\n",
    "5. **Agglomerative Clustering**\n",
    "   - Agglomerative Clustering is performed on the full data matrix to group the data into 3 clusters.\n",
    "\n",
    "6. **Save Results**\n",
    "   - The clustering results are saved to CSV files, including individual files for each cluster label.\n",
    "   - Models and labels are saved to pickle files for future use.\n",
    "\n",
    "7. **Evaluation**\n",
    "   - Evaluation metrics, including Silhouette Score, Calinski-Harabasz Score, and Davies-Bouldin Index, are computed on a subset of the data to assess clustering quality.\n",
    "\n",
    "## Files Generated\n",
    "- `tweets_with_agg_labels.csv`: Data with Agglomerative Clustering labels.\n",
    "- `tweets_label_0.csv`, `tweets_label_1.csv`, `tweets_label_2.csv`: Filtered data by cluster labels.\n",
    "- `svd_model.pkl`: Saved TruncatedSVD model.\n",
    "- `agglomerative_clustering_labels.pkl`: Saved Agglomerative Clustering labels.\n",
    "- `affinity_propagation_labels.pkl`: Saved Affinity Propagation labels.\n",
    "- `cluster_labels.pkl`: Saved labels from Affinity Propagation.\n",
    "- `affinity_propagation_model.pkl`: Saved Affinity Propagation model.\n",
    "\n",
    "## Metrics\n",
    "- **Silhouette Score:** Measures the quality of clustering.\n",
    "- **Calinski-Harabasz Score:** Evaluates cluster separation.\n",
    "- **Davies-Bouldin Index:** Assesses the average similarity ratio of each cluster with its most similar cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db65e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd488d4a",
   "metadata": {
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1721996305912,
     "user": {
      "displayName": "MAYIMUNAH NAGAYI",
      "userId": "13400004802396886361"
     },
     "user_tz": -120
    },
    "id": "cd488d4a"
   },
   "outputs": [],
   "source": [
    "# AP_clustering_implementation.ipynb\n",
    "\n",
    "# Importing Necessary Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.cluster import AffinityPropagation, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, accuracy_score, classification_report, davies_bouldin_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f464a64",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1721995988646,
     "user": {
      "displayName": "MAYIMUNAH NAGAYI",
      "userId": "13400004802396886361"
     },
     "user_tz": -120
    },
    "id": "1f464a64"
   },
   "outputs": [],
   "source": [
    "# Loading the TF-IDF matrix\n",
    "with open('tfidf_matrix.pkl', 'rb') as f:\n",
    "    tfidf_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be23bde7-65e2-428f-8604-980397ea6260",
   "metadata": {
    "executionInfo": {
     "elapsed": 3386,
     "status": "ok",
     "timestamp": 1721995992025,
     "user": {
      "displayName": "MAYIMUNAH NAGAYI",
      "userId": "13400004802396886361"
     },
     "user_tz": -120
    },
    "id": "be23bde7-65e2-428f-8604-980397ea6260"
   },
   "outputs": [],
   "source": [
    "# Reducing dimensionality\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "reduced_tfidf_matrix = svd.fit_transform(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "465233ac-91c0-4882-983e-d52e34ce46d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105485,
     "status": "ok",
     "timestamp": 1721996432698,
     "user": {
      "displayName": "MAYIMUNAH NAGAYI",
      "userId": "13400004802396886361"
     },
     "user_tz": -120
    },
    "id": "465233ac-91c0-4882-983e-d52e34ce46d9",
    "outputId": "90a0d390-e5d7-4aa2-dde3-928042108a84",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to split data into chunks\n",
    "def chunk_data(data, chunk_size):\n",
    "    for i in range(0, data.shape[0], chunk_size):\n",
    "        yield data[i:i + chunk_size]\n",
    "\n",
    "# Initialize variables\n",
    "chunk_size = 1000  # Adjust chunk size as needed\n",
    "all_labels = []  # List to store labels for each chunk\n",
    "all_data_chunks = []  # List to store data chunks for Agglomerative Clustering\n",
    "\n",
    "\n",
    "# Processing data in chunks\n",
    "for chunk in chunk_data(reduced_tfidf_matrix, chunk_size):\n",
    "    affinity_propagation = AffinityPropagation(random_state=4, max_iter=500, damping=0.9)\n",
    "    chunk_labels = affinity_propagation.fit_predict(chunk)\n",
    "    all_labels.append(chunk_labels)\n",
    "    all_data_chunks.append(chunk)  # Collect data for the final clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42c1d95-71b3-446e-aee3-41cfec7ac1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data shape: (27981, 100)\n",
      "Length of labels: 27981\n",
      "Length of Agglomerative labels: 27981\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all data chunks\n",
    "full_data_matrix = np.vstack(all_data_chunks)\n",
    "print(f\"Full data shape: {full_data_matrix.shape}\")\n",
    "\n",
    "# Verify length of labels\n",
    "labels = np.concatenate(all_labels)\n",
    "print(f\"Length of labels: {len(labels)}\")\n",
    "\n",
    "# Perform Agglomerative Clustering on the full data\n",
    "agglomerative_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "agg_labels = agglomerative_clustering.fit_predict(full_data_matrix)\n",
    "\n",
    "# Verify length of Agglomerative labels\n",
    "print(f\"Length of Agglomerative labels: {len(agg_labels)}\")\n",
    "\n",
    "#  save the full dataset with labels\n",
    "df = pd.read_csv('cleaned_tweets.csv')  # Load your tweet data\n",
    "df['agg_labels'] = agg_labels  # Add Agglomerative labels to the DataFrame\n",
    "df.to_csv('tweets_with_agg_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b93dcf4-6c96-42d8-b3b9-9d649e1b48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and save tweets with label 0\n",
    "df_label_0 = df[df['agg_labels'] == 0]\n",
    "df_label_0.to_csv('tweets_label_0.csv', index=False)\n",
    "\n",
    "# Filter and save tweets with label 1\n",
    "df_label_1 = df[df['agg_labels'] == 1]\n",
    "df_label_1.to_csv('tweets_label_1.csv', index=False)\n",
    "\n",
    "# Filter and save tweets with label 2\n",
    "df_label_3 = df[df['agg_labels'] == 2]\n",
    "df_label_3.to_csv('tweets_label_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85cd58b3-3ff8-4743-891f-5da8e79cead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cluster labels and the modelS \n",
    "# Save the SVD model after fitting during training\n",
    "with open('svd_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svd, f)\n",
    "\n",
    "# Save the Agglomerative Clustering labels\n",
    "with open('agglomerative_clustering_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(agg_labels, f)\n",
    "    \n",
    "cluster_labels = affinity_propagation.labels_\n",
    "with open('affinity_propagation_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_labels, f)\n",
    "    \n",
    "with open('cluster_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(labels, f)\n",
    "with open('affinity_propagation_model.pkl', 'wb') as f:\n",
    "    pickle.dump(affinity_propagation, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9f273f1-18d4-4de5-aefe-cd0570e2ece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.169\n",
      "Calinski-Harabasz Score: 13.605\n",
      "Davies-Bouldin Index: 1.953\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on a subset of the data\n",
    "subset_size = 500  # Adjust subset size as needed\n",
    "tfidf_matrix_subset = full_data_matrix[:subset_size, :]\n",
    "agg_labels_subset = agg_labels[:subset_size]\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "silhouette_avg = silhouette_score(tfidf_matrix_subset, agg_labels_subset)\n",
    "calinski_harabasz = calinski_harabasz_score(tfidf_matrix_subset, agg_labels_subset)\n",
    "\n",
    "print(f'Silhouette Score: {silhouette_avg:.3f}')\n",
    "print(f'Calinski-Harabasz Score: {calinski_harabasz:.3f}')\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "davies_bouldin = davies_bouldin_score(tfidf_matrix_subset, agg_labels_subset)\n",
    "print(f'Davies-Bouldin Index: {davies_bouldin:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f1c4d33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1721997663102,
     "user": {
      "displayName": "MAYIMUNAH NAGAYI",
      "userId": "13400004802396886361"
     },
     "user_tz": -120
    },
    "id": "5f1c4d33",
    "outputId": "9b31ed00-4908-44bc-992d-4f9d52fec1fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 3\n"
     ]
    }
   ],
   "source": [
    "# Display the number of clusters\n",
    "n_clusters = len(set(agg_labels))\n",
    "print(f'Number of clusters: {n_clusters}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e08d58-f00a-4e91-941d-9b7dbd407100",
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "aborted",
     "timestamp": 1721996095674,
     "user": {
      "displayName": "MAYIMUNAH NAGAYI",
      "userId": "13400004802396886361"
     },
     "user_tz": -120
    },
    "id": "96e08d58-f00a-4e91-941d-9b7dbd407100"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
