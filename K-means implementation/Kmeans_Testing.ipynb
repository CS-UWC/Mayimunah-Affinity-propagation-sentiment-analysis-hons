{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a338b0de-10c9-44e8-ae73-f478a7a4b88e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9ccc0b6-aab2-44a0-8ba3-6f34e6d594f8",
   "metadata": {},
   "source": [
    "# KMeans Clustering Testing\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the application of a pre-trained KMeans clustering model on new tweet data. It covers text preprocessing, feature extraction, dimensionality reduction, clustering, and saving results.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Data Preparation**: Define a set of new tweets for clustering.\n",
    "2. **Text Preprocessing**: Clean and tokenize the tweet texts to remove special characters and stopwords.\n",
    "3. **Feature Extraction**: Transform the preprocessed tweets using a pre-trained TF-IDF vectorizer.\n",
    "4. **Dimensionality Reduction**: Apply a pre-trained TruncatedSVD model to reduce the feature dimensions.\n",
    "5. **Clustering**: Use a pre-trained KMeans model to predict cluster labels for the new tweets.\n",
    "6. **Results**: Save the cluster labels and the results to files for future use and optionally export them to a CSV file.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The notebook aims to evaluate how new data fits into the existing clustering framework by using saved models and tools. This process helps in understanding the distribution of new data points within the predefined clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd44f8-caf8-4dc8-b5d2-f8f5e711f3be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0ffafaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Honours\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d64342c-0934-443a-ab5b-987f5250293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved models\n",
    "with open('kmeans_model.pkl', 'rb') as f:\n",
    "    kmeans = pickle.load(f)\n",
    "with open('svd_model.pkl', 'rb') as f:\n",
    "    svd = pickle.load(f)\n",
    "with open('vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1e811c9-5d77-4fdd-b60f-b9de6dd8f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text cleaning, tokenization, and normalization\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove special characters and punctuation\n",
    "    text = re.sub(r'@user', '', text)  # Remove mentions of @user\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16ad8f85-6c88-4810-b1ed-29f0ffa7251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess new tweets\n",
    "new_tweets = [\n",
    "    \"Just saw the most amazing movie! Highly recommend it to everyone.\",\n",
    "    \"I can't believe the weather today—it's raining cats and dogs!\",\n",
    "    \"Feeling so grateful for my friends and family. Life is good.\",\n",
    "    \"This new phone I bought is such a disappointment. Wish I had researched more.\",\n",
    "    \"Had a fantastic workout session at the gym today. Feeling energized!\"\n",
    "]\n",
    "preprocessed_tweets = [clean_text(tweet) for tweet in new_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2aa6774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>KMeans_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just saw the most amazing movie! Highly recomm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't believe the weather today—it's raining...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feeling so grateful for my friends and family....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This new phone I bought is such a disappointme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Had a fantastic workout session at the gym tod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  KMeans_Label\n",
       "0  Just saw the most amazing movie! Highly recomm...             0\n",
       "1  I can't believe the weather today—it's raining...             0\n",
       "2  Feeling so grateful for my friends and family....             1\n",
       "3  This new phone I bought is such a disappointme...             0\n",
       "4  Had a fantastic workout session at the gym tod...             0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform new tweets using the saved TF-IDF vectorizer\n",
    "new_tweets_tfidf = tfidf_vectorizer.transform(preprocessed_tweets)\n",
    "\n",
    "# Apply the SVD transformation to the new data\n",
    "new_tweets_reduced = svd.transform(new_tweets_tfidf)\n",
    "\n",
    "# # Predict the clusters for the new data (WITH MODEL1) and just MODEL\n",
    "#kmeans_labels = kmeans.predict(new_tweets_reduced)\n",
    "\n",
    "#Predict the clusters for the new data (without SVD) MODEL2\n",
    "kmeans_labels = kmeans.predict(new_tweets_tfidf.astype(np.float32).toarray())\n",
    "\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "df_results = pd.DataFrame({\n",
    "    'Tweet': new_tweets,\n",
    "    'KMeans_Label': kmeans_labels\n",
    "})\n",
    "\n",
    "# Display results\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb6c9d2e-566a-4d39-9796-ba9895e7f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the new KMeans cluster labels for further use\n",
    "# with open('new_kmeans_labels.pkl', 'wb') as f:\n",
    "#     pickle.dump(kmeans_labels, f)\n",
    "\n",
    "# # Optionally, save results to a CSV file\n",
    "# df_results.to_csv('tweets_with_kmeans_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b99c422-65cb-4e09-b172-d3c7c11b99ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9e2d0-c5c6-4b66-a545-a923ec0474a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6b438-62ec-4244-bbf4-7d8445a6bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Load new tweets\n",
    "# new_tweets = [\n",
    "#     \"Just saw the most amazing movie! Highly recommend it to everyone.\",\n",
    "#     \"I can't believe the weather today—it's raining cats and dogs!\",\n",
    "#     \"Feeling so grateful for my friends and family. Life is good.\",\n",
    "#     \"This new phone I bought is such a disappointment. Wish I had researched more.\",\n",
    "#     \"Had a fantastic workout session at the gym today. Feeling energized!\"\n",
    "# ]\n",
    "\n",
    "# # Function for text cleaning, tokenization, and normalization\n",
    "# def clean_text(text):\n",
    "#     text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove special characters and punctuation\n",
    "#     text = re.sub(r'@user', '', text)  # Remove mentions of @user\n",
    "#     tokens = word_tokenize(text)  # Tokenize text\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [word.lower() for word in tokens if word.lower() not in stop_words]  # Remove stopwords\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "# # Preprocess new tweets\n",
    "# preprocessed_tweets = [clean_text(tweet) for tweet in new_tweets]\n",
    "\n",
    "# # Load the saved TF-IDF vectorizer\n",
    "# with open('vectorizer.pkl', 'rb') as f:\n",
    "#     tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "# # Transform new tweets using the loaded vectorizer\n",
    "# new_tweets_tfidf = tfidf_vectorizer.transform(preprocessed_tweets)\n",
    "\n",
    "# # Load the saved SVD model (TruncatedSVD for dimensionality reduction)\n",
    "# with open('svd_model.pkl', 'rb') as f:\n",
    "#     svd = pickle.load(f)\n",
    "\n",
    "# # Apply the SVD transformation to the new data (reduce dimensionality)\n",
    "# new_tweets_reduced = svd.transform(new_tweets_tfidf)\n",
    "\n",
    "# # Load the pre-trained KMeans model\n",
    "# with open('kmeans_model.pkl', 'rb') as f:\n",
    "#     kmeans = pickle.load(f)\n",
    "\n",
    "# # Check the type of loaded model\n",
    "# print(type(kmeans))  # Ensure this is <class 'sklearn.cluster._kmeans.KMeans'>\n",
    "\n",
    "# # Predict the clusters for the new data\n",
    "# kmeans_labels = kmeans.predict(new_tweets_reduced)\n",
    "\n",
    "# # Create a DataFrame to store results\n",
    "# df_results = pd.DataFrame({\n",
    "#     'Tweet': new_tweets,\n",
    "#     'KMeans_Label': kmeans_labels\n",
    "# })\n",
    "\n",
    "# # Display results\n",
    "# print(df_results)\n",
    "\n",
    "# # Save the new KMeans cluster labels for further use\n",
    "# with open('new_kmeans_labels.pkl', 'wb') as f:\n",
    "#     pickle.dump(kmeans_labels, f)\n",
    "\n",
    "# # Optionally, save results to a CSV file\n",
    "# df_results.to_csv('tweets_with_kmeans_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293dfd81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
